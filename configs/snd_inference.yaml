# 模型相关配置
# model_name_or_path: "/data/name_disambiguation/model/Qwen/Qwen2.5-3B-Instruct"
model_name_or_path: "/data/name_disambiguation/model/Alibaba-NLP/gte-Qwen2-7B-instruct"
padding_side: "left"
# use_graph: true
use_lora: false
dynamic_weight: false
enable_quantization: false
# lora_module_path: "/data/name_disambiguation/model/3B-Qwen-1Pos-16Pack-0.05Temp-lastPooling-4e-06LR-Shuffled-ClusterLoss-LossWeight2.0/checkpoint-1800/lora.safetensors"
# graph_proj_module_path: "/data/name_disambiguation/model/1.5B-Qwen-1Pos-36Pack-0.05Temp-0.99Decay-200DecayStep-1e-05LR-Sequential-Graph/checkpoint-200/graph_proj.safetensors"
graph_hidden_size: 512

# 数据相关配置
packing_size: 2
positive_num: 1
shuffle: false
mode: "private"
graph_feature_path: "/data/name_disambiguation/data/SND/pub_graph_embs.safetensors" 

# 训练流程控制
do_predict: false
resume_from_checkpoint: "/data/name_disambiguation/model/SNDPacking/checkpoint-1"
num_train_epochs: 1
save_steps: 200
save_total_limit: 200
evaluation_strategy: "steps"
eval_steps: 200
load_best_model_at_end: false
metric_for_best_model: "avg_f1"
remove_unused_columns: false
use_cluster_loss: false

# 优化器相关
learning_rate: 0.00001
warmup_ratio: 0.1
lr_scheduler_type: "cosine"

# 聚类相关参数
db_min: 5
db_eps: 0.05

# 对比学习温度参数
temperature: 0.05
temperature_decay: 0.99
temperature_decay_step: 200

# 硬件/性能配置
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
bf16: true

# 日志和输出
output_dir: "/data/name_disambiguation/model"
report_to: "wandb"
logging_steps: 1